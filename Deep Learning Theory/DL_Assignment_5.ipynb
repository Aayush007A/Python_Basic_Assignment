{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210db974",
   "metadata": {},
   "outputs": [],
   "source": [
    "********************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f2255",
   "metadata": {},
   "source": [
    "## 1. Why would you want to use the Data API?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5e857",
   "metadata": {},
   "source": [
    "APIs are needed to bring applications together in order to perform a designed function built around sharing data and executing pre-defined processes. They work as the middle man, allowing developers to build new programmatic interactions between the various applications people and businesses use on a daily basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "********************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af193c3",
   "metadata": {},
   "source": [
    "## 2. What are the benefits of splitting a large dataset into multiple files?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee82c4",
   "metadata": {},
   "source": [
    "There are 6 Key Benefits of Splitting Your Access Database\n",
    "1. Multiple Users can Access Data Simultaneously. ...\n",
    "2. Provides Better Protection. ...\n",
    "3. Allows for Future Planning. ...\n",
    "4. Easy to Modify User Interface. ...\n",
    "5. Split Databases Can Get Corrupted Due to Access Conflicts. ...\n",
    "6. Author Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "********************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c5199",
   "metadata": {},
   "source": [
    "## 3. During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c11efb",
   "metadata": {},
   "source": [
    "Bottlenecks are setbacks or obstacles that slow or delay a process. In the same way that the neck of a physical bottle will limit how quickly water can pass through it, process bottlenecks can restrict the flow of information, materials, products, and employee hours.\n",
    "\n",
    "Bottlenecks are commonly associated with manufacturing and logistics. But they can occur in any process where networks of people and tasks rely upon one another to keep the work flowing.\n",
    "\n",
    "There are two main types of bottlenecks:\n",
    "\n",
    "Short-term bottlenecks. These are caused by temporary problems. For example, a key team member becomes ill or goes on vacation. No one else is qualified to run their projects, which causes a backlog of work until they return.\n",
    "Long-term bottlenecks. These are the blockages that occur regularly. For example, a company's month-end reporting process is delayed every month, because a specific person has to complete a series of time-consuming tasks first.\n",
    "Both types of bottleneck can lead to lost revenue, dissatisfied customers, poor-quality products or services, and stress for team members, so identifying and fixing them is vital.\n",
    "\n",
    "How to Identify Bottlenecks\n",
    "Identifying bottlenecks in manufacturing is usually pretty easy. For example, you can clearly see when products pile up on an assembly line. In business processes, however, they can be harder to spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4540759",
   "metadata": {},
   "outputs": [],
   "source": [
    "********************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96618690",
   "metadata": {},
   "source": [
    "## 4. Can you save any binary data to a TFRecord file, or only serialized protocol buffers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8b9dc",
   "metadata": {},
   "source": [
    "A TFRecord is when a sequence of such records serializes to binary. The binary format takes less memory for storage in comparison to any other data formats.\n",
    "\n",
    "That’s what I’m going to do now. I will convert all the records of a dataset to TFRecords which can be serialized into binary and can be written in a file. Tensorflow says that,\n",
    "\n",
    "For each value in a record, create an instance of tf.train.Feature.\n",
    "Add each tf.train.Feature instance of a record to a map as a value, with the attribute name (column name) as the key. It will create a tf.train.Features map for a record of the dataset\n",
    "Create an instance of tf.train.Example using the tf.train.Features map created above. With this process, the number of instances of tf.train.Example created will be the same as the number of records in the dataset.\n",
    "I have two demos following on just to show this approach. One of them will be on a csv file and another one would be an image dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85dc578",
   "metadata": {},
   "outputs": [],
   "source": [
    "********************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74a342",
   "metadata": {},
   "source": [
    "## 5. Why would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81321ddd",
   "metadata": {},
   "source": [
    "Protocol buffers are a method of serializing data that can be transmitted over wire or be stored in files. The other formats like JSON and XML are also used for serializing data. Although these platforms have proved themselves to be extremely flexible and effective, one place where they aren’t fully optimized is scenarios where the data is to be transmitted between multiple microservices in a platform-neutral way.\n",
    "\n",
    "This was the challenge that made Google create the ProtoBuf format in 2008. Since then, it’s widely been used internally at Google and has been the default data format for the gRPC framework. Initially, the Protobuf was created for three primary languages — C++, Java, and Python. Over the course of years, many languages like Go, Ruby, JS, PHP, C#, and Objective-C have also started supporting Protobufs. The current version of Protobuf is called proto3.\n",
    "\n",
    "Like JSON and XML, the Protobufs are language and platform-neutral. The Protobuf is optimized to be faster than JSON and XML by removing many responsibilities usually done by data formats and making it focus only on the ability to serialize and deserialize data as fast as possible. Another important optimization is regarding how much network bandwidth is being utilized by making the transmitted data as small as possible.\n",
    "\n",
    "The definition of the data to be serialized is written in configuration files called proto files (.proto). These files will contain the configurations known as messages. The proto files can be compiled to generate the code in the user's programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "********************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1133005",
   "metadata": {},
   "source": [
    "## 6. When using TFRecords, when would you want to activate compression? Why not do it systematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edb3ec",
   "metadata": {},
   "source": [
    "I have two demos following on just to show this approach. One of them will be on a csv file and another one would be an image dataset. But before diving into that, just a few more things to clear.\n",
    "\n",
    "1. I need a method to create appropriate tf.train.Feature depending on the type of the value in the record. So here I’ve written a single method which can be used for both the examples.\n",
    "\n",
    "The method here takes rows from a dataset, and its column types and column names as lists.\n",
    "It then iterates on the values of the row, checks the type, and then creates the respective tf.train.Feature element.\n",
    "Then as explained above, add the tf.train.Feature instance to a tf.train.Features map as a value against the column name as a key. The map is then used to create an instance of tf.train.Example.\n",
    "\n",
    "2. I need another method to parse the tfrecords while reading the serialized formats. Each column has its own parsing mechanism based on its type.\n",
    "\n",
    "This shows the parsing mechanism of each attribute while reading from a tfrecord.\n",
    "It takes a map of the column names and column types as key-value pairs.\n",
    "It then uses tf.io.FixedLenFeature configuration to parse fixed length input features, with the respective types of the values. This populates another map with the name of the columns as the keys and the parsers created as the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe60b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "********************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c836f",
   "metadata": {},
   "source": [
    "## 7. Data can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d075e8d",
   "metadata": {},
   "source": [
    "While most Machine Learning programmers start with common open-source datasets like MNIST or CIFAR-10, and that’s all well and good, but to expand your horizon, and solve problems, you need to go beyond these and get your own data. While the collection of data may or may not be too hard, most people find difficulties in making this data ready for training. This is mostly because of the large number of intermediate steps like format conversion (usually for Computer Vision), Tokenizing (for NLP) and the general steps of Data Augmentation, Shuffling etc.\n",
    "\n",
    "### Steps involved in a Data Pipeline\n",
    "\n",
    "_Extract_ the dataset from its compressed form, after selecting which dataset or raw data you’re going to use. The format of the data extracted plays an important role in the next step.\n",
    "\n",
    "_Transform_ your data into suitable formats for a Deep Learning Model, i.e., into numeric form, and apply required preprocessing steps like Data Augmentation, shuffling and batching.\n",
    "\n",
    "Simply Load your database into the workspace and use it in your Machine Learning Model for training and testing.\n",
    "This ETL approach is common to all Data Pipelines, and the ML Pipeline is no exception.\n",
    "\n",
    "Doing these steps using individual frameworks can be tedious, which is exactly where TensorFlow Datasets or TFDS and the tf.data module fit in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "********************************************************************************************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
